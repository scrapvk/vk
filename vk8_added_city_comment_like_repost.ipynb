{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vk\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, tqdm_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opennig vk session with access token.\n",
    "session = vk.Session(access_token='0dfed272d8c2c0882f26fabaee86c1c23eca901348c1e8476f93a1fd83fe06b4b9fc4070f7c0f1aa269f6')\n",
    "api = vk.API(session)\n",
    "# example use with id, my id\n",
    "api.users.get(user_ids='architectofreality' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for a keyword glaukoma starting 48 hours ago, max 200 counts can be scrapped at once\n",
    "glaukoma=api.newsfeed.search(q='glaukoma',count=200, start_time=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get last 200 posts --> take the time of the last post --> at the next iteration set the end time = time_last_post-1. Set the start time to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "glaukoma_rus=api.newsfeed.search(q='glaukoma',count=200, end_time=1512378120, start_time=1512378121-86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the first element and creat a dataframe with the rest\n",
    "df=pd.DataFrame(glaukoma_rus[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting info of each user, deactivated column\n",
    "ids=pd.DataFrame(api.users.get(user_ids=abs(df['owner_id'])))\n",
    "ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_users_info(x):\n",
    "    # x is api newsfeed search result\n",
    "    # use each user only once to get personal info (different posts from same user have the same personal info)\n",
    "    unique_users = list(set(x['owner_id'].astype(int)))\n",
    "    users = pd.DataFrame(api.users.get(user_ids=unique_users,fields = ['occupation','education',\n",
    "                                                                       'universities','bdate',\n",
    "                                                                       'city','interests',\n",
    "                                                                       'activities','site']))\n",
    "\n",
    "    try:     \n",
    "        # fill active users status with valid\n",
    "        users['deactivated'].fillna('Valid',inplace = True) \n",
    "        # select all deactivated user ids\n",
    "        deactivated_ids = list(users[users['deactivated']!='Valid'].uid)\n",
    "\n",
    "        # select all active user posts\n",
    "        active_users = x[-x.owner_id.isin(deactivated_ids)]\n",
    "        active_users_info = users[users['deactivated']=='Valid']\n",
    "    except:\n",
    "        active_users = x  \n",
    "        active_users_info = users\n",
    "\n",
    "    # merge post info and user info\n",
    "    overall_data = active_users.merge(active_users_info, left_on='owner_id', right_on='uid')\n",
    "    return overall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "key_words = ['#глаза',\n",
    "             '#глазки', '#зрение', '#лечениезрения', '#детскоезрение', '#лечениеглаз',\n",
    "             '#офтальмолог','#офтальмология', '#офтальмохирург', '#офтальмохирургия', '#детскоезрение',\n",
    "             '#детскийофтальмолог', '#офтальмологмосква','#дакриоцистит', '#дакриоциститноворожденных',\n",
    "             '#дакриоциститмассаж','#дакриоциститлечение', '#дакриоцисториностомия', '#зондированиеносослёзногоканала',\n",
    "             '#зондированиеноворождённых','#нистагм', '#лечениенистагма','#нистагмоперация', '#нистагмъ',\n",
    "             '#близорукость', '#миопия', '#лечениеблизорукости', '#близорукостьнеприговор',\n",
    "             '#близоркостьлечение', '#близорукостьонатакая', '#близорукостьнепорок',\n",
    "             '#близорукостьудетей','#дальнозоркость', '#гиперметропия', '#лечениедальнозоркости',\n",
    "             '#дальнозоркостьлечение','#дальнозоркостьудетей', '#дальнозоркости','#косоглазие',\n",
    "             '#страбизм', '#лечениекосоглазия', '#косоглазиелечение', '#косоглазиеэтосексуально',\n",
    "             '#косоглазиебишкек', '#косоглазиенепорок', '#косоглазие дети', '#косоглазиеэтомодно',\n",
    "             '#астигматизм', '#астигматизмлечение', '#астигматизмудетей','#амблиопия', '#амблиопиявысокойстепени',\n",
    "             '#амблиопиямосква', '#катаракта','#катарактавладивосток', '#катарактачелябинск',\n",
    "             '#катаракта лечение москва','#катарактаоперация', '#глаукома', '#глаукомалечение', \n",
    "             '#глаукомавладивосток','#глаукомачелябинск', '#птоз', '#птозверхнеговека', '#птозвека',\n",
    "             '#птозвек', '#птозанет','#лазернаякоррекция', '#лазернаякоррекциязрения', '#лазернаякоррекциязрениямосква',\n",
    "             '#лазернаякоррекцияакция', '#лазернаякоррекциямосква', '#лазернаякоррекциязренияомикрон',\n",
    "             '#лазернаякоррекцияскидки', '#lasik', '#фрк', '#ласик', '#ласикмосква', '#ячмень', '#халязион']\n",
    "desired_time = 86400*365 # how much back we want to go in seconds\n",
    "entire_df = pd.DataFrame()\n",
    "# Overall iteration count\n",
    "k = 1\n",
    "for key_word in key_words:\n",
    "    print('search for keyword: ', key_word)\n",
    "    date = time.time() # now\n",
    "    excess_time = True\n",
    "    all_df = pd.DataFrame()\n",
    "    # Iteration count for each key word\n",
    "    c=1\n",
    "    # Iterator for error count (TimeOut Error)\n",
    "    error = 0\n",
    "    while excess_time:\n",
    "        # in case a ReadTimeOut Error occurs\n",
    "        try:\n",
    "            # Print which iteration it is for current key word   \n",
    "            print(c,'iteration')\n",
    "\n",
    "            '''\n",
    "            # Resting each overall 60 iterations\n",
    "            if k%60 == 0:\n",
    "                print('Rest for 10 min')\n",
    "                time.sleep(60*10)\n",
    "                print('Work')\n",
    "            '''\n",
    "\n",
    "            # print iteration count, extended tells us group vs user\n",
    "            newsfeed_search_results = api.newsfeed.search(q=[key_word],count=200, end_time = date-1, start_time=1,extended=1)\n",
    "            nsr_df = pd.DataFrame(newsfeed_search_results[1:])\n",
    "            # Filter posts done by groups\n",
    "            try:\n",
    "                nsr_df = nsr_df[nsr_df.owner_id>0].reset_index()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if len(nsr_df)>0: # check if g contains data\n",
    "                new_data = active_users_info(nsr_df)\n",
    "                all_df = pd.concat([all_df,new_data])\n",
    "                date=newsfeed_search_results[-1]['date']\n",
    "                time.sleep(31) # Sleep time to not exceed requests of the same method\n",
    "\n",
    "            elif len(nsr_df) <= 0 and c == 1: # check if there is data\n",
    "                print('No posts with that keyword(s)!')\n",
    "                time.sleep(31) # Sleep time to not exceed requests of the same method\n",
    "                break\n",
    "\n",
    "            elif len(nsr_df) <= 0 and c != 1: # check if there are posts eft to scrape\n",
    "                print('No more posts to scrape!')\n",
    "                time.sleep(31) # Sleep time to not exceed requests of the same method\n",
    "                break\n",
    "\n",
    "            # increment the iteration counts\n",
    "            c+=1\n",
    "            k+=1\n",
    "\n",
    "            if date<time.time()-desired_time: # check termination condition\n",
    "                excess_time = False\n",
    "        # just repeat the request\n",
    "        except:\n",
    "            print('Timeout Error: sleep for 2 min')\n",
    "            time.sleep(60*2)\n",
    "            error += 1\n",
    "            if error == 2:\n",
    "                print('Encountered TimeOut Error 2 times')\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    all_df['keyword'] = key_word\n",
    "    print('all_df length: ',len(all_df))\n",
    "    entire_df = pd.concat([entire_df,all_df])\n",
    "    \n",
    "\n",
    "# getting list of cities by their ids\n",
    "all_city_ids = entire_df.city.unique().astype(int)\n",
    "city_name = []\n",
    "# cant scrape more than 1000 in 1 request\n",
    "for cities in range(int(len(all_city_ids)/1000)+1):\n",
    "    city = api.database.getCitiesById(city_ids = all_city_ids[cities*1000:(cities+1)*1000])\n",
    "    city_name = city_name+city\n",
    "\n",
    "# creating dataframe from the list of the cities    \n",
    "entire_df['city'] = entire_df['city'].astype(int)\n",
    "# merging 2 dataframes on city id\n",
    "entire_df = entire_df.merge(pd.DataFrame(city_name), left_on='city', right_on='cid')\n",
    "\n",
    "print('entire_df length: ',len(entire_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df.to_csv(r'D:\\vk_all_keywors_365_days_vk8_10Feb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "overall_data=entire_df.copy()\n",
    "# creating a dataframe for dictionary values\n",
    "from tqdm import tqdm\n",
    "new_df = pd.DataFrame(columns = ['post_id','user_id','post','sex','Name_Surname',\n",
    "                                 'likes','comments','reposts','platform','date','key_word',\n",
    "                                 'city','university','graduation_year','faculty_name',\n",
    "                                 'eduaction_status','activities','interests','work_type',\n",
    "                                 'work_place','site','bdate'])\n",
    "# resetting the index it make circles of 0-200\n",
    "#active_users.reset_index(inplace = True)\n",
    "    \n",
    "# making function to apply to get platform\n",
    "def platform(x):\n",
    "    try:\n",
    "        return x.post_source['type'] + '_' + x.post_source['platform']\n",
    "    except:\n",
    "        return x.post_source['type']\n",
    "\n",
    "# making function to get work type    \n",
    "def type_scrap(x):\n",
    "    try:\n",
    "        return x['type']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# making function to get work place   \n",
    "def place_scrap(x):\n",
    "    try:\n",
    "        return x['name']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "new_df['post_id'] = overall_data['id']\n",
    "new_df['platform'] = overall_data.apply(platform, axis = 1)\n",
    "new_df['key_word'] = overall_data['keyword']\n",
    "new_df['user_id'] = overall_data['user'].apply(lambda x: x['uid'])   \n",
    "new_df['Name_Surname'] = overall_data['user'].apply(lambda x: x['first_name'])+'_'+overall_data['user'].apply(lambda x: x['last_name'])\n",
    "new_df['post'] = overall_data['text']\n",
    "new_df['likes_count'] = overall_data['likes'].apply(lambda x: x['count'])\n",
    "new_df['comments_count'] = overall_data['comments'].apply(lambda x: x['count']) \n",
    "new_df['reposts_count'] = overall_data['reposts'].apply(lambda x: x['count'])\n",
    "new_df['date'] = overall_data['date'].apply(lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "new_df['sex'] = overall_data['user'].apply(lambda x: 'Female' if x['sex']==1 else 'Male' if x['sex']==2  else 'Not Specified')\n",
    "new_df['city'] = overall_data['name']\n",
    "new_df['university'] = overall_data['university_name']\n",
    "new_df['gradutaion_year'] = overall_data['graduation']#.astype(int) #some nan value occur\n",
    "new_df['faculty_name'] = overall_data['faculty_name']\n",
    "new_df['education_status'] = overall_data['education_status']\n",
    "new_df['activities'] = overall_data['activities']\n",
    "new_df['interests'] = overall_data['interests']\n",
    "new_df['work_type'] = overall_data['occupation'].apply(type_scrap)\n",
    "new_df['work_place'] = overall_data['occupation'].apply(place_scrap)\n",
    "new_df['site'] = overall_data['site']\n",
    "new_df['bdate'] = overall_data['bdate']\n",
    "#new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'D:\\vk_all_keywors_365_days_vk8_10Feb_pretty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the comments of posts\n",
    "def commentscrapper(x):\n",
    "    uid = x['user_id']\n",
    "    pid = x['post_id']\n",
    "    likes = api.likes.getList(type='post', item_id = pid, owner_id = uid)\n",
    "    time.sleep(3)\n",
    "    reposts = api.likes.getList(type='post', item_id = pid, owner_id = uid, filter = 'copies')\n",
    "    time.sleep(3)\n",
    "    try:           \n",
    "        coms = api.wall.getComments(owner_id = uid, post_id = pid, need_likes = 1)\n",
    "        time.sleep(3)\n",
    "        return {'comments':coms[1:],'users_liked': likes,'users_reposted': reposts,'user_id': uid, 'post_id':pid}\n",
    "    except:\n",
    "        return {'comments':'Access Denied','users_liked':likes,'users_reposted': reposts,'user_id': uid, 'post_id':pid}\n",
    "        time.sleep(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_data3 = new_df.head().copy()#pd.DataFrame()\n",
    "clr_data3['comments'] = None\n",
    "clr_data3['users_liked'] = None\n",
    "clr_data3['users_reposted'] = None\n",
    "for i in tqdm(range(new_df.head().shape[0])):\n",
    "    clr = commentscrapper(new_df.iloc[i])\n",
    "    clr_data3.set_value(i,'comments' ,clr['comments'] )     \n",
    "    clr_data3.set_value(i,'users_liked', clr['users_liked'])          \n",
    "    clr_data3.set_value(i,'users_reposted', clr['users_reposted'])    \n",
    "\n",
    "#final_data = pd.concat([new_df,clr_data],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_data.to_csv('clr_demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 45\n",
    "short_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove posts with more than 300 characters and posts with no characters\n",
    "short_df['post']=short_df.text.apply(lambda x: x if len(x)<300 and len(x) !=0 else 'Too Long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "not_long = short_df[short_df['post']!='Too Long'].reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "list_of_animals = 'кот|кошка|котёнок|котяра|котяры|котяр|котище|котик|собака|пес|собачка|кобель|псина|сука|птица|пташка|птенцы|птенец|щенок'\n",
    "def find_animal(x):\n",
    "    my_list = re.findall(list_of_animals,x['post'])\n",
    "    if len(my_list) == 0:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['animal'] = new_df.apply(find_animal,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_df.animal == True).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.to_csv(r'D:\\vk_all_keywords_365_days_short.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
